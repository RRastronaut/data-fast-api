{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import PIL\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "\n",
    "from sklearn import set_config; set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**All data**\n",
    "\n",
    "WagonCab's engineering team stores all it's cab course history since 2009 in a massive Big Query table `wagon-public-datasets.taxifare.raw_all`. \n",
    "- This table contains `1.1 Million` for this challenge exactly, from **2009 to jun 2015**. \n",
    "- *(Note from Le Wagon: In reality, there is 55M rows but we limited that for cost-control in the whole module)*\n",
    "\n",
    "**Training phase data**\n",
    "\n",
    "- The training phase takes place in `2015-01-01`. We can't access any data beyond that date to train our model\n",
    "- The datascientist's notebook hereby trains a ML model on a **200k randomly sampled subset** (so that everything fit in RAM memory on the Data Scientist's laptop)\n",
    "- WagonCab's data-engineering team has created a `materialized view` of this `raw` table called `raw_200k` and given read access to the DataScientist for its training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATA_PATH = Path('~').joinpath(\".lewagon\", \"mlops\", \"data\").expanduser()\n",
    "GCP_PROJECT_WAGON = \"wagon-public-datasets\"\n",
    "BQ_DATASET = \"taxifare\"\n",
    "DATA_SIZE = \"200k\"  # raw_200k is a randomly sampled materialized view from \"raw_all\" data table\n",
    "MIN_DATE = '2009-01-01'\n",
    "MAX_DATE = '2015-01-01'\n",
    "COLUMN_NAMES_RAW = ('fare_amount',\t'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fill your `GCP_PROJECT` below üëá**, Then let's query the historical data (pre-2015), ordered by date (so as to train/test split chronologically more easily) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECT = \"<your gcp project id>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT {\",\".join(COLUMN_NAMES_RAW)}\n",
    "    FROM {GCP_PROJECT_WAGON}.{BQ_DATASET}.raw_{DATA_SIZE} \n",
    "    WHERE pickup_datetime BETWEEN '{MIN_DATE}' AND '{MAX_DATE}'\n",
    "    ORDER BY pickup_datetime\n",
    "    \"\"\"\n",
    "print(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the BigQuery call only if file does not already exist locally.  \n",
    "Else, store locally as CSV to avoid calling BigQuery at each notebook run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_query_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"raw\", f\"query_{MIN_DATE}_{MAX_DATE}_{DATA_SIZE}.csv\")\n",
    "\n",
    "if data_query_cache_path.is_file():\n",
    "    print(\"load local file...\")\n",
    "    df = pd.read_csv(data_query_cache_path, parse_dates=['pickup_datetime'])\n",
    "    \n",
    "else:\n",
    "    print(\"Querying Big Query server...\")\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    client = bigquery.Client(project=GCP_PROJECT)\n",
    "    query_job = client.query(query)\n",
    "    result = query_job.result() \n",
    "    df = result.to_dataframe()\n",
    "    \n",
    "    df.to_csv(data_query_cache_path, header=True, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1) Compress Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compress our DataFrame by lowering its numeric `dtypes`\n",
    "- from  `float64` to `float32`\n",
    "- from `int64` to `int8`\n",
    "\n",
    "To do so, we iterate on its columns, and for each one, reduce its `dtypes` as much as possible using [`pd.to_numeric`](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html)\n",
    "\n",
    "**üí° 1) Read more about `dtype` compression in the ML Ops - Train at Scale lecture on Kitt, \"Appendix A1: Memory Optimization\"**\n",
    "\n",
    "**üí° 2) Then, understand and execute the following code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(df, **kwargs):\n",
    "    \"\"\"\n",
    "    Reduces the size of the DataFrame by downcasting numerical columns\n",
    "    \"\"\"\n",
    "    input_size = df.memory_usage(index=True).sum()/ 1024**2\n",
    "    print(\"old dataframe size: \", round(input_size,2), 'MB')\n",
    "    \n",
    "    in_size = df.memory_usage(index=True).sum()\n",
    "\n",
    "    for t in [\"float\", \"integer\"]:\n",
    "        l_cols = list(df.select_dtypes(include=t))\n",
    "\n",
    "        for col in l_cols:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=t)\n",
    "\n",
    "    out_size = df.memory_usage(index=True).sum()\n",
    "    ratio = (1 - round(out_size / in_size, 2)) * 100\n",
    "    \n",
    "    print(\"optimized size by {} %\".format(round(ratio,2)))\n",
    "    print(\"new DataFrame size: \", round(out_size / 1024**2,2), \" MB\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compress(df, verbose=True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check dtypes optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Int8 is a bit too small for some scientific libraries, let's use int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.passenger_count = df.passenger_count.astype(\"int16\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Next time, we can force dtypes directly **at loading time** as follow:\n",
    "\n",
    "```python\n",
    "query_job = client.query(query)\n",
    "result = query_job.result() \n",
    "df = result.to_dataframe(dtypes=DTYPES_RAW)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPES_RAW = {\n",
    "    \"key\": \"datetime64[ns, UTC]\",\n",
    "    \"fare_amount\": \"float32\",\n",
    "    \"pickup_datetime\": \"datetime64[ns, UTC]\",\n",
    "    \"pickup_longitude\": \"float32\",\n",
    "    \"pickup_latitude\": \"float32\",\n",
    "    \"dropoff_longitude\": \"float32\",\n",
    "    \"dropoff_latitude\": \"float32\",\n",
    "    \"passenger_count\": \"int16\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove redundant rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove buggy transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='any', axis=0)\n",
    "print(df.shape)\n",
    "df = df[(df.dropoff_latitude != 0) | (df.dropoff_longitude != 0) | (df.pickup_latitude != 0) | (df.pickup_longitude != 0)]\n",
    "df = df[df.passenger_count > 0]\n",
    "df = df[df.fare_amount > 0]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove geographically irrelevant transactions (rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check NYC bouding boxes\n",
    "# Load image of NYC map\n",
    "bounding_boxes = (-74.3, -73.7, 40.5, 40.9)\n",
    "\n",
    "url = 'https://wagon-public-datasets.s3.amazonaws.com/data-science-images/07-ML-OPS/nyc_-74.3_-73.7_40.5_40.9.png'\n",
    "nyc_map = np.array(PIL.Image.open(urllib.request.urlopen(url)))\n",
    "\n",
    "plt.imshow(nyc_map);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"pickup_latitude\"].between(left=40.5, right=40.9)]\n",
    "df = df[df[\"dropoff_latitude\"].between(left=40.5, right=40.9)]\n",
    "df = df[df[\"pickup_longitude\"].between(left=-74.3, right=-73.7)]\n",
    "df = df[df[\"dropoff_longitude\"].between(left=-74.3, right=-73.7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's cap training set to reasonable values \n",
    "df = df[df.fare_amount < 400]\n",
    "df = df[df.passenger_count < 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of fare\n",
    "df.fare_amount.hist(bins=100, figsize=(14,3))\n",
    "plt.xlabel('fare $USD')\n",
    "plt.title('Histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be used more often to plot data on the NYC map\n",
    "def plot_on_map(df, BB, nyc_map, s=10, alpha=0.2):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16,10))\n",
    "\n",
    "    axs[0].scatter(df.pickup_longitude, df.pickup_latitude, zorder=1, alpha=alpha, c='red', s=s)\n",
    "    axs[0].set_xlim((BB[0], BB[1]))\n",
    "    axs[0].set_ylim((BB[2], BB[3]))\n",
    "    axs[0].set_title('Pickup locations')\n",
    "    axs[0].imshow(nyc_map, zorder=0, extent=BB)\n",
    "\n",
    "    axs[1].scatter(df.dropoff_longitude, df.dropoff_latitude, zorder=1, alpha=alpha, c='blue', s=s)\n",
    "    axs[1].set_xlim((BB[0], BB[1]))\n",
    "    axs[1].set_ylim((BB[2], BB[3]))\n",
    "    axs[1].set_title('Dropoff locations')\n",
    "    axs[1].imshow(nyc_map, zorder=0, extent=BB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training data on map\n",
    "plot_on_map(df, bounding_boxes, nyc_map, s=1, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_on_map(df, bounding_boxes, nyc_map, s=20, alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hires(df, BB, figsize=(12, 12), ax=None, c=('r', 'b')):\n",
    "    if ax == None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    \n",
    "    def select_within_boundingbox(df, BB):\n",
    "        return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n",
    "            (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n",
    "            (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n",
    "            (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])\n",
    "            \n",
    "    idx = select_within_boundingbox(df, BB)\n",
    "    ax.scatter(df[idx].pickup_longitude, df[idx].pickup_latitude, c=\"red\", s=0.01, alpha=0.5)\n",
    "    ax.scatter(df[idx].dropoff_longitude, df[idx].dropoff_latitude, c=\"blue\", s=0.01, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hires(df, (-74.1, -73.7, 40.6, 40.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hires(df, (-74, -73.95, 40.7, 40.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Baseline Score  - Preliminary Intuitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A baseline model should at least take into account the most obvious feature: the distance between `pickup` and `dropoff`.\n",
    "\n",
    "The correct distance metric is appropriately named \"Manhattan distance\" (L1 distance), which computes the sum of horizontal and vertical distances between two points, instead of the diagonal (Euclidean, L2) distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def manhattan_distance(start_lat: float, start_lon: float, end_lat: float, end_lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Manhattan distance between in km two points on the earth (specified in decimal degrees).\n",
    "    \"\"\"\n",
    "    earth_radius = 6371\n",
    "    \n",
    "    lat_1_rad, lon_1_rad = math.radians(start_lat), math.radians(start_lon)\n",
    "    lat_2_rad, lon_2_rad = math.radians(end_lat), math.radians(end_lon)\n",
    "    \n",
    "    dlon_rad = lon_2_rad - lon_1_rad\n",
    "    dlat_rad = lat_2_rad - lat_1_rad\n",
    "    \n",
    "    manhattan_rad = abs(dlon_rad) + abs(dlat_rad)\n",
    "    manhattan_km = manhattan_rad * earth_radius\n",
    "    \n",
    "    return manhattan_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.apply(lambda row: manhattan_distance(row[\"pickup_latitude\"], row[\"pickup_longitude\"], row[\"dropoff_latitude\"], row[\"dropoff_longitude\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è The following code takes a while and is not optimized.\n",
    "\n",
    "Applying a mapping function row-by-row on a DataFrame is a very bad engineering practice, as DataFrames are stored in memory \"column-by-column\". We talk about \"column-major\" data storage formats. \n",
    "\n",
    "`df.apply(..., axis=1)` is equivalent to a python `for` loop, and does not harness NumPy's vectorized operations.\n",
    "\n",
    "üëá Let's vectorize our code instead! Notice the improvement by a factor of several hundred üí™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance_vectorized(df: pd.DataFrame, start_lat: str, start_lon: str, end_lat: str, end_lon: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the Manhattan distance in km between two points on the earth (specified in decimal degrees).\n",
    "    Vectorized version for pandas df\n",
    "    \"\"\"\n",
    "    earth_radius = 6371\n",
    "    \n",
    "    lat_1_rad, lon_1_rad = np.radians(df[start_lat]), np.radians(df[start_lon])\n",
    "    lat_2_rad, lon_2_rad = np.radians(df[end_lat]), np.radians(df[end_lon])\n",
    "    \n",
    "    dlon_rad = lon_2_rad - lon_1_rad\n",
    "    dlat_rad = lat_2_rad - lat_1_rad\n",
    "    \n",
    "    manhattan_rad = np.abs(dlon_rad) + np.abs(dlat_rad)\n",
    "    manhattan_km = manhattan_rad * earth_radius\n",
    "    \n",
    "    return manhattan_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "manhattan_distance_vectorized(df, \"pickup_latitude\", \"pickup_longitude\",\"dropoff_latitude\", \"dropoff_longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['distance'] = manhattan_distance_vectorized(df, \"pickup_latitude\", \"pickup_longitude\",\"dropoff_latitude\", \"dropoff_longitude\")\n",
    "df['distance'].hist(bins=50)\n",
    "plt.title(\"distance (km)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the `distance` column we added manually, and create a true preprocessing pipeline now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['distance'])\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Train/Val/Test Split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üö® We're dealing with timestamped data:\n",
    "- It's important to split train/val/test in a **chronological** manner.\n",
    "- We don't want to hold too long val or tests sets: macro-economic conditions are changing fast in real life!\n",
    "- The smaller the `split_ratio`, the more often we'll have to re-train our model\n",
    "- With a dataset of 6 years, it's perfectly fine to keep 1 month ahead of val, 1 month ahead for test\n",
    "- In production, this means we shouldn't trust our model performance to extend beyond 1 month in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.02 # ~1 month for val, ~1 month for test\n",
    "\n",
    "test_length = int(len(df) * split_ratio)\n",
    "val_length = int((len(df)-test_length) * split_ratio)\n",
    "train_length = len(df) - val_length - test_length\n",
    "\n",
    "df_train = df.iloc[:train_length, :].sample(frac=1) # Shuffle datasets to improve training\n",
    "df_val = df.iloc[train_length: train_length + val_length, :].sample(frac=1)\n",
    "df_test = df.iloc[train_length+val_length:, :].sample(frac=1)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "assert len(df_train) + len(df_val) + len(df_test) == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.pickup_datetime.min())\n",
    "print(df_train.pickup_datetime.max())\n",
    "print('---')\n",
    "print(df_val.pickup_datetime.min())\n",
    "print(df_val.pickup_datetime.max())\n",
    "print('---')\n",
    "print(df_test.pickup_datetime.min())\n",
    "print(df_test.pickup_datetime.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(\"fare_amount\", axis=1)\n",
    "y = df[[\"fare_amount\"]]\n",
    "\n",
    "X_train = df_train.drop(\"fare_amount\", axis=1)\n",
    "y_train = df_train[[\"fare_amount\"]]\n",
    "\n",
    "X_val = df_val.drop(\"fare_amount\", axis=1)\n",
    "y_val = df_val[[\"fare_amount\"]]\n",
    "\n",
    "X_test = df_test.drop(\"fare_amount\", axis=1)\n",
    "y_test = df_test[[\"fare_amount\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Baseline: $price = a * distance + b $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_train = np.array(manhattan_distance_vectorized(X_train, \"pickup_latitude\", \"pickup_longitude\",\"dropoff_latitude\", \"dropoff_longitude\"))\n",
    "distances_val = np.array(manhattan_distance_vectorized(X_val, \"pickup_latitude\", \"pickup_longitude\",\"dropoff_latitude\", \"dropoff_longitude\"))\n",
    "distances_test = np.array(manhattan_distance_vectorized(X_test, \"pickup_latitude\", \"pickup_longitude\",\"dropoff_latitude\", \"dropoff_longitude\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=distances_train, y=y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearson, p_value = pearsonr(distances_train, y_train)\n",
    "print(f'{pearson=}')\n",
    "print(f'{p_value=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "baseline_model = LinearRegression()\n",
    "baseline_model.fit(distances_train[:, None], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pred_val = baseline_model.predict(distances_val[:, None])\n",
    "baseline_pred_test = baseline_model.predict(distances_test[:, None])\n",
    "baseline_mae_val = np.mean(np.abs(baseline_pred_val - y_val), axis=0)\n",
    "baseline_mae_test = np.mean(np.abs(baseline_pred_test - y_test), axis=0)\n",
    "\n",
    "print(f'mean taxifare prices on train set = {round(float(np.mean(y_train, axis=0)),2)} $')\n",
    "print(f'üéØ baseline MAE on val set = {round(float(baseline_mae_val),2)} $')\n",
    "print(f'üéØ baseline MAE on test set = {round(float(baseline_mae_test),2)} $')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given a dataset with only 5 features (passengers + lon/lat), and potentially dozens of millions of rows.\n",
    "\n",
    "üëâ It makes perfect sense to create a lot of \"engineered\" features such as \"hour of the day\"  \n",
    "- Hundreds of them would cause no problem because the huge number of rows will allow our model to learn all weights associated with these multiple features\n",
    "- A dense, Deep Learning network will be well-suited for such a case\n",
    "\n",
    "‚ùóÔ∏è The proposed preprocessor below outputs a **fixed number of features** (65) that is **independent of the training set**. You will see that it will come in handy when scaling it up to hundreds of millions of rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Passenger Preprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze passenger numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.passenger_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x=\"passenger_count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSENGER PIPE\n",
    "p_min = 1.\n",
    "p_max = 8.\n",
    "passenger_pipe = FunctionTransformer(lambda p: (p-p_min)/(p_max-p_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passenger_scaler\", passenger_pipe, [\"passenger_count\"]),\n",
    "    ],\n",
    ")\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preprocessor.fit_transform(X_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Time Preprocessor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's extract interesting attributes from the `pickup_datetime`\n",
    "- hour of the day\n",
    "- day of the week\n",
    "- month of the year\n",
    "- number of days since 2009 (it may encode inflation parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def transform_time_features(X: pd.DataFrame) -> np.ndarray:\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "    \n",
    "    timedelta = (X[\"pickup_datetime\"] - pd.Timestamp('2009-01-01T00:00:00', tz='UTC'))/pd.Timedelta(1,'D')\n",
    "    \n",
    "    pickup_dt = X[\"pickup_datetime\"].dt.tz_convert(\"America/New_York\").dt\n",
    "    dow = pickup_dt.weekday\n",
    "    hour = pickup_dt.hour\n",
    "    month = pickup_dt.month\n",
    "\n",
    "    hour_sin = np.sin(2 * math.pi / 24 * hour)\n",
    "    hour_cos = np.cos(2*math.pi / 24 * hour)\n",
    "    \n",
    "    return np.stack([hour_sin, hour_cos, dow, month, timedelta], axis=1)\n",
    "\n",
    "X_time_processed = transform_time_features(X[[\"pickup_datetime\"]])\n",
    "\n",
    "pd.DataFrame(X_time_processed, columns=[\"hour_sin\", \"hour_cos\", \"dow\", \"month\", \"timedelta\"]).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we one-hot-encode `[\"day of week\", \"month\"]` by forcing all 24*7 combinations of categories to be always present in `X_processed` \n",
    "\n",
    "(remember we want a fixed size for `X_processed` at the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_categories = [\n",
    "        np.arange(0, 7, 1),  # days of the week from 0 to 6\n",
    "        np.arange(1, 13, 1)  # months of the year from 1 to 12\n",
    "    ]\n",
    "\n",
    "OneHotEncoder(categories=time_categories, sparse_output=False)\\\n",
    "    .fit_transform(X_time_processed[:,[2,3]]) # column index [2,3] for ['dow', 'month'] !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And combine this with a sort of \"Min-Max\" re-scaling of the `timedelta` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_time_processed[:,-1].min())\n",
    "print(X_time_processed[:,-1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelta_min = 0\n",
    "timedelta_max = 2190 # Our model may extend in the future. No big deal if the scaled data extend slightly beyond 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pipe = make_pipeline(\n",
    "    FunctionTransformer(transform_time_features),\n",
    "    make_column_transformer(\n",
    "        (OneHotEncoder(\n",
    "            categories=time_categories,\n",
    "            sparse_output=False,\n",
    "            handle_unknown=\"ignore\"\n",
    "        ), [2, 3]), # corresponds to columns [\"day of week\", \"month\"], not the other columns\n",
    "\n",
    "        (FunctionTransformer(lambda year: (year - timedelta_min) / (timedelta_max - timedelta_min)), [4]), # min-max scale the columns 4 [\"year\"]\n",
    "        remainder=\"passthrough\" # keep hour_sin and hour_cos\n",
    "    )\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passenger_scaler\", passenger_pipe, [\"passenger_count\"]),\n",
    "        (\"time_preproc\", time_pipe, [\"pickup_datetime\"]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preprocessor.fit_transform(X_train)).plot(kind='box');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è 23 features approximately centered and scaled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) Distance Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add both the haversine and Manhattan distances as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonlat_features = [\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_vectorized(df: pd.DataFrame, start_lat: str, start_lon: str, end_lat: str, end_lon: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the haversine and Manhattan distances between two points (specified in decimal degrees).\n",
    "    Vectorized version for pandas df\n",
    "    Computes distance in Km\n",
    "    \"\"\"\n",
    "    earth_radius = 6371\n",
    "\n",
    "    lat_1_rad, lon_1_rad = np.radians(df[start_lat]), np.radians(df[start_lon])\n",
    "    lat_2_rad, lon_2_rad = np.radians(df[end_lat]), np.radians(df[end_lon])\n",
    "\n",
    "    dlon_rad = lon_2_rad - lon_1_rad\n",
    "    dlat_rad = lat_2_rad - lat_1_rad\n",
    "\n",
    "    manhattan_rad = np.abs(dlon_rad) + np.abs(dlat_rad)\n",
    "    manhattan_km = manhattan_rad * earth_radius\n",
    "\n",
    "    a = (np.sin(dlat_rad / 2.0)**2 + np.cos(lat_1_rad) * np.cos(lat_2_rad) * np.sin(dlon_rad / 2.0)**2)\n",
    "    haversine_rad = 2 * np.arcsin(np.sqrt(a))\n",
    "    haversine_km = haversine_rad * earth_radius\n",
    "\n",
    "    return dict(\n",
    "        haversine = haversine_km,\n",
    "        manhattan = manhattan_km\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_lonlat_features(X:pd.DataFrame)-> pd.DataFrame:\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "    res = distances_vectorized(X, *lonlat_features)\n",
    "\n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "distances = transform_lonlat_features(X[lonlat_features])\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_min = 0\n",
    "dist_max = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_pipe = make_pipeline(\n",
    "    FunctionTransformer(transform_lonlat_features),\n",
    "    FunctionTransformer(lambda dist: (dist - dist_min) / (dist_max - dist_min))\n",
    "    )\n",
    "distance_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passenger_scaler\", passenger_pipe, [\"passenger_count\"]),\n",
    "        (\"time_preproc\", time_pipe, [\"pickup_datetime\"]),\n",
    "        (\"dist_preproc\", distance_pipe, lonlat_features),\n",
    "    ],\n",
    ")\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = pd.DataFrame(preprocessor.fit_transform(X_train))\n",
    "X_processed.plot(kind='box');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è 25 features, approximately scaled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4) GeoHasher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's add information about **districts**! \n",
    "\n",
    "Some might be more expensive than others to go to/depart from (e.g. JFK airport!)\n",
    "\n",
    "To _bucketize_ geospatial information, we'll use `pygeohash`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygeohash as gh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° pygeohash converts (lat,lon) into geospacial \"squared buckets\" of chosen precisions. The more precision you ask, the more \"buckets\" possibility there is!\n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/07-ML-OPS/geohashes.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = X_train.iloc[0,:]\n",
    "(x0.pickup_latitude, x0.pickup_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gh.encode(x0.pickup_latitude, x0.pickup_longitude, precision=3))\n",
    "print(gh.encode(x0.pickup_latitude, x0.pickup_longitude, precision=4))\n",
    "print(gh.encode(x0.pickup_latitude, x0.pickup_longitude, precision=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Let's apply it to ALL of our data set (note that this preprocessing may take a very long time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_geohash(X:pd.DataFrame, precision:int = 5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Add a geohash (ex: \"dr5rx\") of len \"precision\" = 5 by default\n",
    "    corresponding to each (lon, lat) tuple, for pick-up and drop-off\n",
    "    \"\"\"\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "    X[\"geohash_pickup\"] = X.apply(\n",
    "        lambda x: gh.encode(x.pickup_latitude, x.pickup_longitude, precision=precision),\n",
    "        axis=1\n",
    "    )\n",
    "    X[\"geohash_dropoff\"] = X.apply(\n",
    "        lambda x: gh.encode(x.dropoff_latitude, x.dropoff_longitude, precision=precision),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return X[[\"geohash_pickup\", \"geohash_dropoff\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_geohash(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Notice that this time, we have no choice but to apply `pygeohash` row by row with `df.apply(axis=1)`, and it takes a while to compute.\n",
    "\n",
    "This is the danger of relying on an external Python library that is not always vectorized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá What are the most common districts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_geohashes = pd.concat([\n",
    "    X_train.apply(lambda x: gh.encode(x.pickup_latitude, x.pickup_longitude, precision=4), axis=1),\n",
    "    X_train.apply(lambda x: gh.encode(x.dropoff_latitude, x.dropoff_longitude, precision=4), axis=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_geohashes.value_counts()))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(np.cumsum(all_geohashes.value_counts()[:20])/(2*len(X_train))*100)\n",
    "plt.title(\"percentage of taxi rides from/to these districts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Only the first 20 districts matter. We can one-hot encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_geohash_districts = np.array(all_geohashes.value_counts()[:20].index)\n",
    "most_important_geohash_districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's hard-code below the 20 most frequent district GeoHashes of precision 5,\n",
    "# covering about 99% of all dropoff/pickup locations.\n",
    "most_important_geohash_districts = [\n",
    "    \"dr5ru\", \"dr5rs\", \"dr5rv\", \"dr72h\", \"dr72j\", \"dr5re\", \"dr5rk\",\n",
    "    \"dr5rz\", \"dr5ry\", \"dr5rt\", \"dr5rg\", \"dr5x1\", \"dr5x0\", \"dr72m\",\n",
    "    \"dr5rm\", \"dr5rx\", \"dr5x2\", \"dr5rw\", \"dr5rh\", \"dr5x8\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's one-hot encode each GeoHash in one of the top-20 different buckets listed above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geohash_categories = [\n",
    "    most_important_geohash_districts,  # pickup district list\n",
    "    most_important_geohash_districts  # dropoff district list\n",
    "]\n",
    "\n",
    "geohash_pipe = make_pipeline(\n",
    "    FunctionTransformer(compute_geohash),\n",
    "    OneHotEncoder(\n",
    "        categories=geohash_categories,\n",
    "        handle_unknown=\"ignore\",\n",
    "        sparse_output=False\n",
    "    )\n",
    ")\n",
    "geohash_pipe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5) Full Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap our final preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygeohash as gh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_time_features(X: pd.DataFrame) -> np.ndarray:\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "    \n",
    "    timedelta = (X[\"pickup_datetime\"] - pd.Timestamp('2009-01-01T00:00:00', tz='UTC'))/pd.Timedelta(1,'D')\n",
    "    \n",
    "    pickup_dt = X[\"pickup_datetime\"].dt.tz_convert(\"America/New_York\").dt\n",
    "    dow = pickup_dt.weekday\n",
    "    hour = pickup_dt.hour\n",
    "    month = pickup_dt.month\n",
    "\n",
    "    hour_sin = np.sin(2 * math.pi / 24 * hour)\n",
    "    hour_cos = np.cos(2*math.pi / 24 * hour)\n",
    "    \n",
    "    return np.stack([hour_sin, hour_cos, dow, month, timedelta], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_lonlat_features(X: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "    lonlat_features = [\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"]\n",
    "\n",
    "    def distances_vectorized(df: pd.DataFrame, start_lat: str, start_lon: str, end_lat: str, end_lon: str) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate the haversine and Manhattan distances between two points on the earth (specified in decimal degrees).\n",
    "        Vectorized version for pandas df\n",
    "        Computes distance in Km\n",
    "        \"\"\"\n",
    "        earth_radius = 6371\n",
    "\n",
    "        lat_1_rad, lon_1_rad = np.radians(df[start_lat]), np.radians(df[start_lon])\n",
    "        lat_2_rad, lon_2_rad = np.radians(df[end_lat]), np.radians(df[end_lon])\n",
    "\n",
    "        dlon_rad = lon_2_rad - lon_1_rad\n",
    "        dlat_rad = lat_2_rad - lat_1_rad\n",
    "\n",
    "        manhattan_rad = np.abs(dlon_rad) + np.abs(dlat_rad)\n",
    "        manhattan_km = manhattan_rad * earth_radius\n",
    "\n",
    "        a = (np.sin(dlat_rad / 2.0)**2 + np.cos(lat_1_rad) * np.cos(lat_2_rad) * np.sin(dlon_rad / 2.0)**2)\n",
    "        haversine_rad = 2 * np.arcsin(np.sqrt(a))\n",
    "        haversine_km = haversine_rad * earth_radius\n",
    "\n",
    "        return dict(\n",
    "            haversine=haversine_km,\n",
    "            manhattan=manhattan_km)\n",
    "\n",
    "    result = pd.DataFrame(distances_vectorized(X, *lonlat_features))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_geohash(X: pd.DataFrame, precision: int = 5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Add a GeoHash (ex: \"dr5rx\") of len \"precision\" = 5 by default\n",
    "    corresponding to each (lon, lat) tuple, for pick-up, and drop-off\n",
    "    \"\"\"\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "    X[\"geohash_pickup\"] = X.apply(\n",
    "        lambda x: gh.encode(x.pickup_latitude, x.pickup_longitude, precision=precision),\n",
    "        axis=1\n",
    "    )\n",
    "    X[\"geohash_dropoff\"] = X.apply(\n",
    "        lambda x: gh.encode(x.dropoff_latitude, x.dropoff_longitude, precision=precision),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return X[[\"geohash_pickup\", \"geohash_dropoff\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSENGER PIPE\n",
    "p_min = 1\n",
    "p_max = 8\n",
    "passenger_pipe = FunctionTransformer(lambda p: (p - p_min) / (p_max - p_min))\n",
    "\n",
    "# DISTANCE PIPE\n",
    "dist_min = 0\n",
    "dist_max = 100\n",
    "\n",
    "distance_pipe = make_pipeline(\n",
    "    FunctionTransformer(transform_lonlat_features),\n",
    "    FunctionTransformer(lambda dist: (dist - dist_min) / (dist_max - dist_min))\n",
    ")\n",
    "\n",
    "# TIME PIPE\n",
    "timedelta_min = 0\n",
    "timedelta_max = 2090\n",
    "\n",
    "time_categories = [\n",
    "    np.arange(0, 7, 1),  # days of the week\n",
    "    np.arange(1, 13, 1)  # months of the year\n",
    "]\n",
    "\n",
    "time_pipe = make_pipeline(\n",
    "    FunctionTransformer(transform_time_features),\n",
    "    make_column_transformer(\n",
    "        (OneHotEncoder(\n",
    "            categories=time_categories,\n",
    "            sparse_output=False,\n",
    "            handle_unknown=\"ignore\"\n",
    "        ), [2,3]), # corresponds to columns [\"day of week\", \"month\"], not the other columns\n",
    "\n",
    "        (FunctionTransformer(lambda year: (year - timedelta_min) / (timedelta_max - timedelta_min)), [4]), # min-max scale the columns 4 [\"timedelta\"]\n",
    "        remainder=\"passthrough\" # keep hour_sin and hour_cos\n",
    "    )\n",
    ")\n",
    "\n",
    "# GEOHASH PIPE\n",
    "lonlat_features = [\n",
    "    \"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\",\n",
    "    \"dropoff_longitude\"\n",
    "]\n",
    "\n",
    "# Below are the 20 most frequent district geohashes of precision 5,\n",
    "# covering about 99% of all dropoff/pickup locations,\n",
    "# according to prior analysis in a separate notebook\n",
    "most_important_geohash_districts = [\n",
    "    \"dr5ru\", \"dr5rs\", \"dr5rv\", \"dr72h\", \"dr72j\", \"dr5re\", \"dr5rk\",\n",
    "    \"dr5rz\", \"dr5ry\", \"dr5rt\", \"dr5rg\", \"dr5x1\", \"dr5x0\", \"dr72m\",\n",
    "    \"dr5rm\", \"dr5rx\", \"dr5x2\", \"dr5rw\", \"dr5rh\", \"dr5x8\"\n",
    "]\n",
    "\n",
    "geohash_categories = [\n",
    "    most_important_geohash_districts,  # pickup district list\n",
    "    most_important_geohash_districts  # dropoff district list\n",
    "]\n",
    "\n",
    "geohash_pipe = make_pipeline(\n",
    "    FunctionTransformer(compute_geohash),\n",
    "    OneHotEncoder(\n",
    "        categories=geohash_categories,\n",
    "        handle_unknown=\"ignore\",\n",
    "        sparse_output=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# COMBINED PREPROCESSOR\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passenger_scaler\", passenger_pipe, [\"passenger_count\"]),\n",
    "        (\"time_preproc\", time_pipe, [\"pickup_datetime\"]),\n",
    "        (\"dist_preproc\", distance_pipe, lonlat_features),\n",
    "        (\"geohash\", geohash_pipe, lonlat_features),\n",
    "    ],\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preprocessor.fit(X_train)\n",
    "\n",
    "X_train_processed = final_preprocessor.transform(X_train)\n",
    "X_val_processed = final_preprocessor.transform(X_val)\n",
    "X_test_processed = final_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "pd.DataFrame(X_train_processed).plot(kind='box', ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.heatmap(pd.DataFrame(X_train_processed).corr(), vmin=-1, cmap='RdBu');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, we can can probably also compress our processed data to float32 without loosing too much perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_processed.nbytes / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress the data a bit\n",
    "X_train_processed = X_train_processed.astype(np.float32)\n",
    "X_val_processed = X_val_processed.astype(np.float32)\n",
    "X_test_processed = X_test_processed.astype(np.float32)\n",
    "\n",
    "print(X_train_processed.nbytes / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_processed).describe().mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è The preprocessor outputs a **fixed** number of features (65) that is independent of the training set. \n",
    "\n",
    "‚òùÔ∏è The preprocessor is also  **state-less** (i.e it has no `.fit()` method, only a `.transform()`). It can be seen as a *pure function* $f:X \\rightarrow X_{processed}$ without an internal state, as opposed to standard scaling for instance, which has to store \"X_train standard deviations\" as internal states.\n",
    "\n",
    "These two features will make work much easier for the ML Engineering team to scale preprocessing to hundreds of GBs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import Model, Sequential, layers, regularizers\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(input_shape:tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Initialize the Neural Network with random weights\n",
    "    \"\"\"\n",
    "\n",
    "    reg = regularizers.l1_l2(l1=0.005)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    model.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=reg))\n",
    "    model.add(layers.BatchNormalization(momentum=0.9))\n",
    "    model.add(layers.Dropout(rate=0.1))\n",
    "    model.add(layers.Dense(50, activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization(momentum=0.9))  # use momentum=0 to only use statistic of the last seen minibatch in inference mode (\"short memory\"). Use 1 to average statistics of all seen batch during training histories.\n",
    "    model.add(layers.Dropout(rate=0.1))\n",
    "    model.add(layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "    print(\"‚úÖ model initialized\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(input_shape=X_train_processed.shape[1:])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "batch_size = 256\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    "    restore_best_weights=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_processed,\n",
    "    y_train,\n",
    "    validation_data=(X_val_processed, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE val\", round(model.evaluate(X_val_processed, y_val)[1],2), ' $')\n",
    "print(\"MAE test\", round(model.evaluate(X_test_processed, y_test)[1],2), ' $')\n",
    "print(\"MAE test baseline\", round(float(baseline_mae_test),2), ' $')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.hist(y_pred, label='pred', color='r', bins=200, alpha=0.3)\n",
    "plt.hist(y_test, label='truth', color='b', bins=200, alpha=0.3)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim((0,60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_pred - y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_pred - y_test\n",
    "sns.histplot(residuals)\n",
    "plt.xlim(xmin=-20, xmax=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.sort_values(by='fare_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual vs. actual scatter plot\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.scatter(x=y_test,y=residuals, alpha=0.1)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('residuals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual vs. predicted scatter plot\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.scatter(x=y_pred,y=residuals, alpha=0.1)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('residuals')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Our model trained on a limited 60k dataset has an MAE of about 2.5$ per course, compared with a mean course price of 11$.  \n",
    "\n",
    "A simple linear regression would give us about 3$ of MAE, but the devil lies in the details!\n",
    "\n",
    "In particular, we're not that good at predicting very long/expensive courses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Test Your Understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Try to answer these questions with your buddy\n",
    "1. Are you satisfied with the model's performance? \n",
    "2. Any ideas to improve it?\n",
    "3. What is a stateless pipeline (as opposed to a stateful one)?\n",
    "4. How does an OHEncoder work with fixed column categories?\n",
    "5. How is the data normalization done in the Neural Net?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary markdown='span'>üí° Answers</summary>\n",
    "\n",
    "1. We have a 25% improvement over the linear regression baseline (but we should cross-validate that to be sure). Besides, a 2$ forecast error on taxi courses whose prices also depend on traffic seems close to the irreducible error rate.\n",
    "\n",
    "\n",
    "2. We could improve model by adding more train data (will test that later). Another promising idea could be to *learn* the embedding of our categorical features (instead of one-hot-encoding them).\n",
    "\n",
    "\n",
    "3. A stateless pipeline has no real `.fit()` method, only a `.transform()`. \n",
    "\n",
    "\n",
    "4. To become stateless, we've hard-coded the `categories` to one-hot encode `OneHotEncoder(categories=categories,...)` and hard-coded the statistical features of each column in our scalers:  `FunctionTransformer(lambda dist: (dist - dist_min)/(dist_max - dist_min))`\n",
    "\n",
    "\n",
    "5. In the TensorFlow model, notice the `layers.BatchNormalization()` we've added between each dense layer, which normalizes data batch-per-batch! It's a cool feature to help fix vanish gradients during the back-propagation!\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Predict the price for this new course `X_new` below and store the result `y_new` as a `np.ndarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[0:2,:]['pickup_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pd.DataFrame(dict(\n",
    "    pickup_datetime=[pd.Timestamp(\"2013-07-06 17:18:00\", tz='UTC')],\n",
    "    pickup_longitude=[-73.950655],\n",
    "    pickup_latitude=[40.783282],\n",
    "    dropoff_longitude=[-73.984365],\n",
    "    dropoff_latitude=[40.769802],\n",
    "    passenger_count=[1],\n",
    "))\n",
    "\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute y_new\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'notebook',\n",
    "    subdir='train_at_scale',\n",
    "    y_new=y_new\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run `make test_kitt` so as to allow Kitt to track your progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd .. && make test_kitt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
